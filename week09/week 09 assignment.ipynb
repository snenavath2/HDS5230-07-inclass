{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sanjay Nenavath\n",
        "\n",
        "\n",
        "Week 09 Assignment - Machine Learning with Scikit-learn"
      ],
      "metadata": {
        "id": "68zbXZcWZxd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Using the provided Python notebook different models were deployed for mortality prediction of patients by running basic logistic regression and regularized logistic regression with adjusted penalty terms and random forest system tests. Performance examinations showed basic logistic regression and logistic regression with L1 penalty (C=10) performed at the highest level by attaining identical test accuracy of 0.718. The predictive accuracy level of 0.718 exceeds the null model benchmark value of 0.608.\n",
        "An uncross-validated random forest model showed a significant model overfitting problem through mismatched training accuracy (0.9993) and test accuracy (0.686). The model shows strong evidence of memorizing training data because it fails to learn generalizable patterns. The logistic regression models displayed proper weighting between training and test outcome performance which implies superior generalization capabilities.\n",
        "\n",
        "Among the examined models the logistic regression model with L1 penalty set to 10 displayed minimal superiority because it demonstrated the highest training accuracy at 0.7347 and test accuracy at 0.718. The implementation of the L1 penalty helped both identify important predictors and amplify them while decreasing the importance of unimportant variables. The overall best model detected in the notebook achieves its position through its interpretability together with reasonable computational demands and its identified superiority in predictive performance.\n",
        "\n",
        "\n",
        "Overall proof can be found in the notebook via detailed performance analytics of each evaluated model. The results indicate the Logistic_L1_C_10 model and basic Logistic model shared the same 0.718 test accuracy yet the L1-penalized version had a slightly better training accuracy at 0.7347 compared to 0.7333. The results confirmed the predictive power of these models since they surpassed the null model baseline (0.608 test accuracy). The Random Forest model demonstrated classic overfitting signs by attaining a training accuracy of 0.9993 yet producing an unsatisfying test accuracy of 0.686 which constitutes a significant 0.3133 difference indicating poor ability to generalize. The regularized logistic regression models with variable C values had comparable results to the C=10 model yet displayed slightly worse performance indicating it achieved best stats for the dataset. Among the evaluated models the prediction performance combined with generalization ability was highest in the Logistic_L1_C_10 model."
      ],
      "metadata": {
        "id": "IGGt_57RZ8Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2 #3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from patsy import dmatrices\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "df_patient = pd.read_csv('PatientAnalyticFile.csv')\n",
        "\n",
        "df_patient['mortality'] = np.where(df_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "df_patient['DateOfBirth'] = pd.to_datetime(df_patient['DateOfBirth'])\n",
        "df_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
        "vars_remove = ['PatientID', 'First_Appointment_Date', 'DateOfBirth',\n",
        "               'Last_Appointment_Date', 'DateOfDeath', 'mortality']\n",
        "vars_left = set(df_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)\n",
        "\n",
        "# Create design matrices\n",
        "y, X = dmatrices(formula, df_patient)\n",
        "y = np.ravel(y)\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "#All solvers with small penalty\n",
        "all_solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "C_value = 1e10\n",
        "\n",
        "# Store results\n",
        "all_results = []\n",
        "\n",
        "for solver in all_solvers:\n",
        "    try:\n",
        "        print(f\"\\nFitting model with solver: {solver}\")\n",
        "\n",
        "        # Create and fit model with small penalty\n",
        "        model = LogisticRegression(penalty='l2', C=C_value, solver=solver, max_iter=2000, random_state=42)\n",
        "\n",
        "        # Time the model fitting\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        fit_time = time.time() - start_time\n",
        "\n",
        "        # Make predictions\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "        # Store results\n",
        "        all_results.append({\n",
        "            'Solver': solver,\n",
        "            'Training Accuracy': train_accuracy,\n",
        "            'Holdout Accuracy': test_accuracy,\n",
        "            'Time (seconds)': fit_time,\n",
        "            'Penalty': f\"L2 (C={C_value})\"\n",
        "        })\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"  Training accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"  Holdout accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"  Fitting time: {fit_time:.4f} seconds\")\n",
        "        print(f\"  Confusion matrix (holdout set):\")\n",
        "        print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error with solver {solver}: {e}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "results_df['Training Accuracy'] = results_df['Training Accuracy'].map(lambda x: f\"{x:.4f}\")\n",
        "results_df['Holdout Accuracy'] = results_df['Holdout Accuracy'].map(lambda x: f\"{x:.4f}\")\n",
        "results_df['Time (seconds)'] = results_df['Time (seconds)'].map(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"\\nSolver Performance Comparison:\")\n",
        "print(results_df[['Solver', 'Training Accuracy', 'Holdout Accuracy', 'Time (seconds)']].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ5HtNfXbz1h",
        "outputId": "f2e3a61c-4f23-4af3-f003-3202c7ee860d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 16000 samples, Test set: 4000 samples\n",
            "\n",
            "Fitting model with solver: newton-cg\n",
            "  Training accuracy: 0.7481\n",
            "  Holdout accuracy: 0.7355\n",
            "  Fitting time: 0.1162 seconds\n",
            "  Confusion matrix (holdout set):\n",
            "[[2139  423]\n",
            " [ 635  803]]\n",
            "\n",
            "Fitting model with solver: lbfgs\n",
            "  Training accuracy: 0.7479\n",
            "  Holdout accuracy: 0.7355\n",
            "  Fitting time: 0.3245 seconds\n",
            "  Confusion matrix (holdout set):\n",
            "[[2139  423]\n",
            " [ 635  803]]\n",
            "\n",
            "Fitting model with solver: liblinear\n",
            "  Training accuracy: 0.7479\n",
            "  Holdout accuracy: 0.7362\n",
            "  Fitting time: 0.0569 seconds\n",
            "  Confusion matrix (holdout set):\n",
            "[[2140  422]\n",
            " [ 633  805]]\n",
            "\n",
            "Fitting model with solver: sag\n",
            "  Training accuracy: 0.7479\n",
            "  Holdout accuracy: 0.7358\n",
            "  Fitting time: 2.6909 seconds\n",
            "  Confusion matrix (holdout set):\n",
            "[[2140  422]\n",
            " [ 635  803]]\n",
            "\n",
            "Fitting model with solver: saga\n",
            "  Training accuracy: 0.7480\n",
            "  Holdout accuracy: 0.7360\n",
            "  Fitting time: 4.8133 seconds\n",
            "  Confusion matrix (holdout set):\n",
            "[[2140  422]\n",
            " [ 634  804]]\n",
            "\n",
            "Solver Performance Comparison:\n",
            "   Solver Training Accuracy Holdout Accuracy Time (seconds)\n",
            "newton-cg            0.7481           0.7355         0.1162\n",
            "    lbfgs            0.7479           0.7355         0.3245\n",
            "liblinear            0.7479           0.7362         0.0569\n",
            "      sag            0.7479           0.7358         2.6909\n",
            "     saga            0.7480           0.7360         4.8133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) The liblinear solver proved to be the most effective solver according to the research results. The evaluation of solver performance considers three metrics including holdout accuracy (generalization performance) and training accuracy and execution time. The evaluation metric of holdout accuracy holds the most importance because it demonstrates how well the model can predict new data points.\n",
        "\n",
        "The liblinear solver produced the optimal holdout accuracy of 0.7362 which outperformed all other solvers including newton-cg, lbfgs, sag, and saga. The best holdout accuracy demonstrates that this solver produces the most effective predictions for unseen data points which represents the fundamental requirement for a well-performing model.\n",
        "\n",
        "The liblinear solver delivered both top holdout accuracy and fast execution time during the fitting process which required 0.0569 seconds. The execution time of 0.0569 seconds for liblinear stands as significantly faster than the execution times of 2.6909 seconds for sag and 4.8133 seconds for saga. The fitting process for newton-cg and lbfgs took longer than liblinear even though these solvers showed better execution times than the rest.\n",
        "\n",
        "All solvers demonstrated equivalent performance in terms of training accuracy since their accuracy levels remained almost identical. The training data fit well for each solver according to the results. The most important metric for selecting the best model is holdout accuracy because training accuracy optimization can lead to overfitting.\n",
        "\n",
        "The logistic regression problem benefits most from the solver liblinear when considering all performance metrics. The solver achieves the best holdout accuracy while maintaining competitive training accuracy and speedier execution than most alternative solvers. The solver demonstrates optimal performance because it achieves both high processing speed and accurate results for this particular dataset."
      ],
      "metadata": {
        "id": "KtwwQ9Ncgdd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-sT_Gp1wa_uS"
      }
    }
  ]
}